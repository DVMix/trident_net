{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !apt install freeglut3-dev -y\n",
    "# !pip3 install tensorflow\n",
    "# !apt-get install -y ffmpeg\n",
    "# !apt-get install -y python3-opengl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from scores.score_logger import ScoreLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 5# 20\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "\n",
    "\n",
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "\n",
    "def cartpole():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    #score_logger = ScoreLogger(ENV_NAME)\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    run = 0\n",
    "    max_score = 0\n",
    "    while True:\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            env.render()\n",
    "            action = dqn_solver.act(state)\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            reward = reward if not terminal else -reward\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                if step>=max_score:\n",
    "                    print(\"Run: \" + str(run) + \", exploration: \" + \n",
    "                          str(dqn_solver.exploration_rate) + \", score: \" + str(step))\n",
    "                    # score_logger.add_score(step, run)\n",
    "                    max_score = step\n",
    "                break\n",
    "            dqn_solver.experience_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "#score_logger = ScoreLogger(ENV_NAME)\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "dqn_solver = DQNSolver(observation_space, action_space)\n",
    "run = 0\n",
    "\n",
    "run += 1\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, observation_space])\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step += 1\n",
    "env.render()\n",
    "# action = dqn_solver.act(state)\n",
    "if np.random.rand() < dqn_solver.exploration_rate:\n",
    "    action = random.randrange(dqn_solver.action_space)\n",
    "else:\n",
    "    q_values = dqn_solver.model.predict(state)\n",
    "    print('q_values =', q_values[0])\n",
    "    action = np.argmax(q_values[0])\n",
    "print('action =', action)\n",
    "\n",
    "state_next, reward, terminal, info = env.step(action)\n",
    "print('cart_position, cart_velocity, pole_angle, pole_velocity')\n",
    "print('state_n = ', state_next)\n",
    "print('reward  = ', reward)\n",
    "print('terminal= ', terminal)\n",
    "print('info    = ', info)\n",
    "\n",
    "reward = reward if not terminal else -reward\n",
    "state_next = np.reshape(state_next, [1, observation_space])\n",
    "dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "state = state_next\n",
    "print('state = ', state)\n",
    "if terminal:\n",
    "    print(\"Run: \" + str(run) + \", exploration: \" + \n",
    "          str(dqn_solver.exploration_rate) + \", score: \" + str(step))\n",
    "    # score_logger.add_score(step, run)\n",
    "    run += 1\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    step = 0\n",
    "    \n",
    "# dqn_solver.experience_replay()\n",
    "if len(dqn_solver.memory) < BATCH_SIZE:\n",
    "    print('collect more samples!')\n",
    "else:\n",
    "    batch = random.sample(dqn_solver.memory, BATCH_SIZE)\n",
    "    for state, action, reward, state_next, terminal in batch:\n",
    "        q_update = reward\n",
    "        if not terminal:\n",
    "            q_update = (reward + GAMMA * np.amax(dqn_solver.model.predict(state_next)[0]))\n",
    "        q_values = dqn_solver.model.predict(state)\n",
    "        q_values[0][action] = q_update\n",
    "        print('q_values =', q_values[0])\n",
    "        dqn_solver.model.fit(state, q_values, verbose=0)\n",
    "    dqn_solver.exploration_rate *= EXPLORATION_DECAY\n",
    "    dqn_solver.exploration_rate = max(EXPLORATION_MIN, dqn_solver.exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Run: 1, exploration: 0.9322301194154049, score: 19\n",
      "Run: 3, exploration: 0.7219385759785162, score: 41\n",
      "Run: 29, exploration: 0.060687903789832374, score: 188\n",
      "Run: 109, exploration: 0.01, score: 271\n",
      "Run: 119, exploration: 0.01, score: 272\n",
      "Run: 137, exploration: 0.01, score: 289\n",
      "Run: 148, exploration: 0.01, score: 322\n",
      "Run: 160, exploration: 0.01, score: 324\n",
      "Run: 185, exploration: 0.01, score: 326\n",
      "Run: 201, exploration: 0.01, score: 367\n",
      "Run: 206, exploration: 0.01, score: 367\n",
      "Run: 213, exploration: 0.01, score: 397\n",
      "Run: 231, exploration: 0.01, score: 495\n",
      "Run: 255, exploration: 0.01, score: 500\n",
      "Run: 260, exploration: 0.01, score: 500\n",
      "Run: 275, exploration: 0.01, score: 500\n",
      "Run: 277, exploration: 0.01, score: 500\n",
      "Run: 288, exploration: 0.01, score: 500\n",
      "Run: 305, exploration: 0.01, score: 500\n",
      "Run: 314, exploration: 0.01, score: 500\n",
      "Run: 329, exploration: 0.01, score: 500\n",
      "Run: 340, exploration: 0.01, score: 500\n",
      "Run: 344, exploration: 0.01, score: 500\n",
      "Run: 345, exploration: 0.01, score: 500\n",
      "Run: 350, exploration: 0.01, score: 500\n",
      "Run: 356, exploration: 0.01, score: 500\n",
      "Run: 360, exploration: 0.01, score: 500\n",
      "Run: 406, exploration: 0.01, score: 500\n",
      "Run: 409, exploration: 0.01, score: 500\n",
      "Run: 417, exploration: 0.01, score: 500\n",
      "Run: 425, exploration: 0.01, score: 500\n",
      "Run: 435, exploration: 0.01, score: 500\n",
      "Run: 436, exploration: 0.01, score: 500\n",
      "Run: 467, exploration: 0.01, score: 500\n",
      "Run: 479, exploration: 0.01, score: 500\n",
      "Run: 482, exploration: 0.01, score: 500\n",
      "Run: 488, exploration: 0.01, score: 500\n",
      "Run: 497, exploration: 0.01, score: 500\n",
      "Run: 498, exploration: 0.01, score: 500\n",
      "Run: 503, exploration: 0.01, score: 500\n",
      "Run: 505, exploration: 0.01, score: 500\n",
      "Run: 514, exploration: 0.01, score: 500\n",
      "Run: 517, exploration: 0.01, score: 500\n",
      "Run: 522, exploration: 0.01, score: 500\n",
      "Run: 526, exploration: 0.01, score: 500\n",
      "Run: 539, exploration: 0.01, score: 500\n",
      "Run: 541, exploration: 0.01, score: 500\n",
      "Run: 606, exploration: 0.01, score: 500\n",
      "Run: 613, exploration: 0.01, score: 500\n",
      "Run: 614, exploration: 0.01, score: 500\n",
      "Run: 638, exploration: 0.01, score: 500\n",
      "Run: 670, exploration: 0.01, score: 500\n",
      "Run: 677, exploration: 0.01, score: 500\n",
      "Run: 719, exploration: 0.01, score: 500\n",
      "Run: 752, exploration: 0.01, score: 500\n",
      "Run: 753, exploration: 0.01, score: 500\n",
      "Run: 762, exploration: 0.01, score: 500\n",
      "Run: 764, exploration: 0.01, score: 500\n",
      "Run: 767, exploration: 0.01, score: 500\n",
      "Run: 781, exploration: 0.01, score: 500\n",
      "Run: 783, exploration: 0.01, score: 500\n",
      "Run: 788, exploration: 0.01, score: 500\n",
      "Run: 789, exploration: 0.01, score: 500\n",
      "Run: 793, exploration: 0.01, score: 500\n",
      "Run: 797, exploration: 0.01, score: 500\n",
      "Run: 805, exploration: 0.01, score: 500\n",
      "Run: 806, exploration: 0.01, score: 500\n",
      "Run: 809, exploration: 0.01, score: 500\n",
      "Run: 811, exploration: 0.01, score: 500\n",
      "Run: 819, exploration: 0.01, score: 500\n",
      "Run: 822, exploration: 0.01, score: 500\n",
      "Run: 832, exploration: 0.01, score: 500\n",
      "Run: 835, exploration: 0.01, score: 500\n",
      "Run: 937, exploration: 0.01, score: 500\n",
      "Run: 938, exploration: 0.01, score: 500\n",
      "Run: 940, exploration: 0.01, score: 500\n",
      "Run: 941, exploration: 0.01, score: 500\n",
      "Run: 950, exploration: 0.01, score: 500\n",
      "Run: 952, exploration: 0.01, score: 500\n",
      "Run: 954, exploration: 0.01, score: 500\n",
      "Run: 1029, exploration: 0.01, score: 500\n",
      "Run: 1045, exploration: 0.01, score: 500\n",
      "Run: 1062, exploration: 0.01, score: 500\n",
      "Run: 1064, exploration: 0.01, score: 500\n",
      "Run: 1112, exploration: 0.01, score: 500\n",
      "Run: 1139, exploration: 0.01, score: 500\n",
      "Run: 1148, exploration: 0.01, score: 500\n",
      "Run: 1170, exploration: 0.01, score: 500\n",
      "Run: 1186, exploration: 0.01, score: 500\n",
      "Run: 1206, exploration: 0.01, score: 500\n",
      "Run: 1223, exploration: 0.01, score: 500\n",
      "Run: 1247, exploration: 0.01, score: 500\n",
      "Run: 1253, exploration: 0.01, score: 500\n",
      "Run: 1254, exploration: 0.01, score: 500\n",
      "Run: 1257, exploration: 0.01, score: 500\n",
      "Run: 1259, exploration: 0.01, score: 500\n",
      "Run: 1271, exploration: 0.01, score: 500\n",
      "Run: 1280, exploration: 0.01, score: 500\n",
      "Run: 1296, exploration: 0.01, score: 500\n",
      "Run: 1298, exploration: 0.01, score: 500\n",
      "Run: 1301, exploration: 0.01, score: 500\n",
      "Run: 1306, exploration: 0.01, score: 500\n",
      "Run: 1314, exploration: 0.01, score: 500\n",
      "Run: 1320, exploration: 0.01, score: 500\n",
      "Run: 1350, exploration: 0.01, score: 500\n",
      "Run: 1403, exploration: 0.01, score: 500\n",
      "Run: 1406, exploration: 0.01, score: 500\n",
      "Run: 1452, exploration: 0.01, score: 500\n",
      "Run: 1459, exploration: 0.01, score: 500\n",
      "Run: 1464, exploration: 0.01, score: 500\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
