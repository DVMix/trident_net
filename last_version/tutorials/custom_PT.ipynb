{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !apt install freeglut3-dev -y\n",
    "# !pip3 install tensorflow\n",
    "# !apt-get install -y ffmpeg\n",
    "# !apt-get install -y python3-opengl\n",
    "# !pip3 install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from DQN import DQN\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ENV_NAME = \"BipedalWalker-v3\"\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 150\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(ENV_NAME)\n",
    "# from gym import envs\n",
    "# for i in envs.registry.all():\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "    def __init__(self, observation_space, action_space, base = True):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.policy_net = DQN(observation_space, action_space, base)\n",
    "#         files = [int(file.split('_')[-1].replace('.pth','')) for file in os.listdir() if '.pth' in file]\n",
    "#         if files!= []:\n",
    "#             file = 'best_'+str(max(files))+'.pth'\n",
    "#             if file in os.listdir():\n",
    "#                 self.policy_net.load_state_dict(torch.load(file))\n",
    "#                 print('Weights loaded!')\n",
    "            \n",
    "        self.target_net = DQN(observation_space, action_space, base)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "                \n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            params=self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.policy_net(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        non_final_mask = ~torch.stack([b[4] for b in batch])\n",
    "        non_final_next_states = torch.stack([b[3] for b in batch if not b[4]])\n",
    "\n",
    "        state_batch  = torch.stack([state[0]  for state  in batch])\n",
    "        action_batch = torch.stack([action[1] for action in batch]).reshape(BATCH_SIZE,1)\n",
    "        reward_batch = torch.stack([reward[2] for reward in batch])\n",
    "\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        loss = self.loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "def cartpole():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    # !!! set observation_space, action_space accuratno\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.shape[0]\n",
    "    dqn_solver = DQNSolver(observation_space, action_space,True)\n",
    "    run = 0\n",
    "    max_rez = 0\n",
    "        \n",
    "    while True:\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(data = state, dtype = torch.float32)\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            env.render()\n",
    "            action = dqn_solver.act(state)\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            reward = reward if not terminal else -reward\n",
    "\n",
    "            action     = torch.tensor(data = action)\n",
    "            reward     = torch.tensor(data = reward,     dtype = torch.float32)\n",
    "            state_next = torch.tensor(data = state_next, dtype = torch.float32)\n",
    "            terminal   = torch.tensor(data = terminal,   dtype = torch.bool)\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next\n",
    "            # print('state = ', state)\n",
    "            if terminal:\n",
    "                if step>=max_rez:\n",
    "                    print(\"Run: \" + str(run) + \", exploration: \" + \n",
    "                          str(dqn_solver.exploration_rate) + \", score: \" + str(step))\n",
    "                    max_rez = step\n",
    "                    if step>=450:\n",
    "                        torch.save(dqn_solver.policy_net.state_dict(), 'best_'+str(step)+'.pth')\n",
    "                        dqn_solver.target_net.load_state_dict(dqn_solver.policy_net.state_dict())\n",
    "                break\n",
    "            dqn_solver.optimize_model()\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
