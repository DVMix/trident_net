{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T11:19:02.251897Z",
     "start_time": "2019-12-17T11:19:01.691217Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T11:19:02.685845Z",
     "start_time": "2019-12-17T11:19:02.253938Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import PrettyPrinter\n",
    "from torch_source import *\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T11:19:02.776227Z",
     "start_time": "2019-12-17T11:19:02.689189Z"
    }
   },
   "outputs": [],
   "source": [
    "import vision.torchvision.models.detection.faster_rcnn as MOD\n",
    "import vision.torchvision.models.detection.backbone_utils as backbone_utils\n",
    "from vision.torchvision.models import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T11:19:02.781589Z",
     "start_time": "2019-12-17T11:19:02.778654Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T11:19:02.813681Z",
     "start_time": "2019-12-17T11:19:02.783786Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch, on_dev):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (images, annotations) in enumerate(tqdm(train_loader, 'Training')): # tqdm(train_loader, 'Training')\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to default device\n",
    "        if on_dev:\n",
    "            if isinstance(images, type(torch.tensor(1))):\n",
    "                images = images.to(device)\n",
    "            else:\n",
    "                images = [img.to(device) for img in images]\n",
    "            annotations = [{key: value.to(device) for key, value in annos.items()} for annos in annotations]\n",
    "        else:\n",
    "            images = images.cuda()\n",
    "            annotations = [{key: value.cuda() for key, value in annos.items()} for annos in annotations]\n",
    "\n",
    "        # Forward prop. Loss\n",
    "        loss = model(images,annotations)\n",
    "        total_los = 3*loss['loss_classifier' ] + \\\n",
    "                    loss['loss_box_reg'    ] + \\\n",
    "                    3*loss['loss_objectness' ] + \\\n",
    "                    loss['loss_rpn_box_reg']\n",
    "        # total_los = total_los.sum()\n",
    "            \n",
    "        # Backward prop.\n",
    "        optimizer.zero_grad()\n",
    "        total_los.backward()\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.update(total_los.item(), len(images)) # .size(0)\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        # Print status\n",
    "        if (i!=0) & (i % print_freq == 0):\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, \n",
    "                                                                  len(train_loader),\n",
    "                                                                  batch_time = batch_time,\n",
    "                                                                  data_time = data_time, \n",
    "                                                                  loss = losses))\n",
    "        # input()\n",
    "    del images, annotations  \n",
    "    \n",
    "    # free some memory since their histories may be stored\n",
    "    \n",
    "def validate(val_loader, model, on_dev):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "\n",
    "    :param val_loader: DataLoader for validation data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :return: average validation loss\n",
    "    \"\"\"\n",
    "    # model.eval()  # eval mode disables dropout\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (images, annotations) in enumerate(tqdm(val_loader,'Validation')):\n",
    "            # Move to default device\n",
    "            if on_dev:\n",
    "                if isinstance(images, type(torch.tensor(1))):\n",
    "                    images = images.to(device)\n",
    "                else:\n",
    "                    images = [img.to(device) for img in images]\n",
    "                annotations = [{key: value.to(device) for key, value in annos.items()} for annos in annotations]\n",
    "            else:\n",
    "                images = images.cuda()\n",
    "                annotations = [{key: value.cuda() for key, value in annos.items()} for annos in annotations]\n",
    "            # Loss\n",
    "            loss = model(images,annotations)\n",
    "#             print(\"loss['loss_classifier' ] = \",loss['loss_classifier' ],\n",
    "#                   \"loss['loss_box_reg'    ] = \",loss['loss_box_reg'    ],\n",
    "#                   \"loss['loss_objectness' ] = \",loss['loss_objectness' ],\n",
    "#                   \"loss['loss_rpn_box_reg'] = \",loss['loss_rpn_box_reg'])\n",
    "            \n",
    "            total_los = loss['loss_classifier' ] + \\\n",
    "                        loss['loss_box_reg'    ] + \\\n",
    "                        loss['loss_objectness' ] + \\\n",
    "                        loss['loss_rpn_box_reg']\n",
    "            #total_los = total_los.sum()\n",
    "            losses.update(total_los.item(), len(images)) # images.size(0)\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if (i!=0) & (i % print_freq == 0):\n",
    "                print('[{0}/{1}]\\t'\n",
    "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(i, len(val_loader),\n",
    "                                                                      batch_time=batch_time,\n",
    "                                                                      loss=losses))\n",
    "\n",
    "    print('\\n * LOSS - {loss.avg:.3f}\\n'.format(loss=losses))\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T11:19:02.831866Z",
     "start_time": "2019-12-17T11:19:02.815619Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "def measure_mAP(model,dataloader_test, data_folder, data_size):\n",
    "    folder_path = os.path.abspath(os.getcwd())\n",
    "    destin_path = 'chkpnts'\n",
    "    destin_folder_path = os.path.join(folder_path, destin_path, data_size)\n",
    "\n",
    "    if not os.path.exists(destin_folder_path):\n",
    "        os.mkdir(destin_folder_path)\n",
    "    \n",
    "    try:\n",
    "        database = pd.read_pickle(os.path.join(data_folder,'mAPs.pkl'))\n",
    "    except:\n",
    "        print('No such file')\n",
    "        database = pd.DataFrame()\n",
    "\n",
    "    with open(os.path.join(data_folder,'label_map.json'), 'r') as j:\n",
    "            label_map = json.load(j)\n",
    "    rev_label_map = {v: k for k, v in label_map.items()}\n",
    "    \n",
    "    files_to_mAP_calc = []\n",
    "    for file in os.listdir():\n",
    "        if ('BAGS_pretren' in file) & ('BEST_' not in file):\n",
    "            files_to_mAP_calc.append(file)\n",
    "    files_to_mAP_calc.sort()\n",
    "    if len(files_to_mAP_calc)!=0:\n",
    "        for checkpoint_filename in files_to_mAP_calc:\n",
    "\n",
    "            shutil.move(os.path.join(folder_path, checkpoint_filename), \n",
    "                        os.path.join(destin_folder_path, checkpoint_filename))\n",
    "\n",
    "            print('processing on file ', checkpoint_filename,'...')\n",
    "            checkpoint = torch.load(os.path.join(destin_folder_path, checkpoint_filename),map_location='cpu')\n",
    "            model.load_state_dict(checkpoint['model'])\n",
    "            del checkpoint\n",
    "            APs, mAP = evaluate(dataloader_test, model, data_folder=data_folder, device = device)\n",
    "            \n",
    "            mAP = pd.DataFrame([{'checkpoint':checkpoint_filename, 'mAP':mAP}])\n",
    "            newline = pd.concat([mAP, pd.DataFrame([APs])], axis = 1)\n",
    "            database = pd.concat([database, newline])\n",
    "        database.to_pickle(os.path.join(data_folder,'mAPs.pkl'))\n",
    "    return newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T11:19:02.941221Z",
     "start_time": "2019-12-17T11:19:02.833870Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(dev = None):\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "    global epochs_since_improvement, start_epoch, label_map, best_loss, epoch, checkpoint\n",
    "    # Initialize model or load checkpoint\n",
    "    model_code = '''\n",
    "        backbone_fpn = backbone_utils.resnet_fpn_backbone('trident_resnet50',pretrained=False)\n",
    "        model = MOD.FasterRCNN(backbone = backbone_fpn, num_classes= 21)'''\n",
    "    rpn_pre_nms_top_n_train=12000 \n",
    "    rpn_pre_nms_top_n_test=12000\n",
    "    rpn_post_nms_top_n_train=500 \n",
    "    rpn_post_nms_top_n_test=500\n",
    "    box_detections_per_img = 20\n",
    "    backbone_fpn = backbone_utils.resnet_fpn_backbone('trident_resnet50', \n",
    "                                                      pretrained=False, \n",
    "                                                      input_channels = 3)#, progress = True)\n",
    "    model = MOD.FasterRCNN(backbone = backbone_fpn, num_classes = 21, \n",
    "                          rpn_pre_nms_top_n_train  = rpn_pre_nms_top_n_train,\n",
    "                          rpn_pre_nms_top_n_test   = rpn_pre_nms_top_n_test,\n",
    "                          rpn_post_nms_top_n_train = rpn_post_nms_top_n_train,\n",
    "                          rpn_post_nms_top_n_test  = rpn_post_nms_top_n_test,\n",
    "                          box_detections_per_img = box_detections_per_img\n",
    "                          )\n",
    "    if checkpoint is None:\n",
    "        model_dict  = model.state_dict()\n",
    "        print('No Checkpoint')\n",
    "        try:\n",
    "            df = pd.read_pickle(os.path.join(data_folder,'mAPs.pkl'))\n",
    "            data = df[df.mAP == df.mAP.max()]\n",
    "            checkpoint = os.path.join(data_size,data.iloc[0][0])\n",
    "            \n",
    "            print('checkpoint - ', checkpoint)\n",
    "            BEST_VOC = 0\n",
    "            if not os.path.exists('chkpnts',data_size, checkpoint):\n",
    "                print('file is not exists! I\"m going to use BEST_VOC0712.pth.tar')\n",
    "                checkpoint = 'BEST_VOC0712.pth.tar'\n",
    "                BEST_VOC = 1\n",
    "            else:\n",
    "                print(\"I'm going to use \",checkpoint)\n",
    "        except:\n",
    "            checkpoint = 'BEST_VOC0712.pth.tar'\n",
    "            BEST_VOC = 1\n",
    "            \n",
    "        faster_dict = torch.load(os.path.join('chkpnts',checkpoint), map_location='cpu')['model']\n",
    "        for key_z in model_dict:\n",
    "            if key_z in faster_dict:\n",
    "                model_dict[key_z] = faster_dict[key_z]\n",
    "        model.load_state_dict(model_dict)\n",
    "        if BEST_VOC ==1:\n",
    "            model.roi_heads.box_predictor.cls_score = torch.nn.Linear(\n",
    "                                    in_features = model.roi_heads.box_predictor.cls_score.in_features, \n",
    "                                    out_features=n_classes)\n",
    "            model.roi_heads.box_predictor.bbox_pred = torch.nn.Linear(\n",
    "                                    in_features = model.roi_heads.box_predictor.bbox_pred.in_features, \n",
    "                                    out_features=n_classes * 4)\n",
    "        biases = list()\n",
    "        not_biases = list()\n",
    "        for param_name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if param_name.endswith('.bias'):\n",
    "                    biases.append(param)\n",
    "                else:\n",
    "                    not_biases.append(param)\n",
    "        optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': lr}, {'params': not_biases}],\n",
    "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        learning_rate = lr\n",
    "    else:\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint, map_location='cpu')\n",
    "        start_epoch = checkpoint['epoch'] \n",
    "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        print('\\nLoaded checkpoint from epoch %d. Best loss is %.3f.\\n' % (start_epoch, best_loss))\n",
    "        print('Do you want to load LR from checkpoint? 1 - Yes, 2 - No')\n",
    "        decision = int(input())\n",
    "        if decision == 1:\n",
    "            learning_rate = checkpoint['lr'];\n",
    "        else:\n",
    "            learning_rate = lr\n",
    "        \n",
    "        optimizer = checkpoint['optimizer']\n",
    "        model.roi_heads.box_predictor.cls_score = torch.nn.Linear(\n",
    "                                    in_features = model.roi_heads.box_predictor.cls_score.in_features, \n",
    "                                    out_features=n_classes)\n",
    "        model.roi_heads.box_predictor.bbox_pred = torch.nn.Linear(\n",
    "                                    in_features = model.roi_heads.box_predictor.bbox_pred.in_features, \n",
    "                                    out_features=n_classes * 4)        \n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        del checkpoint\n",
    "\n",
    " \n",
    "    \n",
    "    # Epochs\n",
    "    \n",
    "    num_to_change = epochs/10\n",
    "    prev_esi = ''\n",
    "    \n",
    "    for i, epoch in enumerate(range(start_epoch, epochs)):\n",
    "        # choose best checkpoint\n",
    "        \n",
    "        dataset_train = PascalVOCDataset(data_folder = data_folder, \n",
    "                                 split = 'train',\n",
    "                                 transforms = get_transform(train=True), max_size = 400) \n",
    "    \n",
    "        dataset_val   = PascalVOCDataset(data_folder = './data/scans/FINAL_TEST/he/',\n",
    "                                         split = 'test',\n",
    "                                         # data_folder = data_folder, \n",
    "                                         # split = 'val',\n",
    "                                         transforms = get_transform(train=False), max_size = 400) \n",
    "\n",
    "        dataset_test  = PascalVOCDataset(data_folder = './data/scans/FINAL_TEST/he/',\n",
    "                                         # data_folder = data_folder, \n",
    "                                         split = 'test',\n",
    "                                         transforms = get_transform(train=False))\n",
    "\n",
    "        dataloader_train = torch.utils.data.DataLoader(dataset_train, \n",
    "                                                       batch_size = batch_size,\n",
    "                                                       shuffle=True, \n",
    "                                                       num_workers=4, \n",
    "                                                       collate_fn=collate_fn) \n",
    "\n",
    "        dataloader_val   = torch.utils.data.DataLoader(dataset_val, \n",
    "                                                       batch_size = batch_size,\n",
    "                                                       shuffle=True, \n",
    "                                                       num_workers=4,\n",
    "                                                       collate_fn=collate_fn)\n",
    "\n",
    "        dataloader_test   = torch.utils.data.DataLoader(dataset_test, \n",
    "                                                       batch_size = test_batch_size, \n",
    "                                                       shuffle=False, \n",
    "                                                       num_workers=4,\n",
    "                                                       collate_fn=collate_fn) \n",
    "        \n",
    "        ch_changer = 0\n",
    "        if i%(num_to_change/5)==0:\n",
    "            ch_changer  = 1\n",
    "            print('Is the way exists? - ',os.path.exists(os.path.join(data_folder,'mAPs.pkl')))\n",
    "            if os.path.exists(os.path.join(data_folder,'mAPs.pkl')):\n",
    "                database = pd.read_pickle(os.path.join(data_folder,'mAPs.pkl'))\n",
    "                chpt = database[database.mAP==database.mAP.max()].checkpoint[0]\n",
    "                print('chpt = ',chpt)\n",
    "                # reload model weights\n",
    "                checkpoint = torch.load(os.path.join('chkpnts',data_size,chpt), map_location='cpu')\n",
    "\n",
    "                diff_obj = {}\n",
    "                rez = database[database.mAP == database.mAP.max()][database.columns[2:]].values[0]\n",
    "                for i, f in enumerate(rez):\n",
    "                    diff_obj[i+1] = f\n",
    "\n",
    "                start_epochs             = checkpoint['epoch']\n",
    "                epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "                best_losses              = checkpoint['best_loss']\n",
    "    #             learning_rate            = checkpoint['lr'];     \n",
    "                model.load_state_dict(checkpoint['model'])\n",
    "                del checkpoint\n",
    "                print('Loaded BEST checkpoint from epoch %d. Best loss is %.3f.\\n' % (start_epochs, best_losses))  \n",
    "        lr_changer = 0\n",
    "        paramparam =  epoch - start_epoch   \n",
    "        if (paramparam!=0)&(paramparam % num_to_change == 0):\n",
    "            lr_changer = 1\n",
    "            learning_rate = learning_rate*0.75;   print('PRES_learning_rate = ', learning_rate)\n",
    "            print('PAST_learning_rate = ', learning_rate)\n",
    "        if (lr_changer == 1)|(ch_changer == 1):\n",
    "            biases = list()\n",
    "            not_biases = list()\n",
    "            for param_name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    if param_name.endswith('.bias'):\n",
    "                        biases.append(param)\n",
    "                    else:\n",
    "                        not_biases.append(param)\n",
    "            optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': learning_rate}, {'params': not_biases}],\n",
    "                                    lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "        if dev is not None:\n",
    "            device = dev\n",
    "            on_dev = True\n",
    "            model = model.to(device)\n",
    "        else:\n",
    "            print('There is NOT any device!')\n",
    "            on_dev = False\n",
    "            cudnn.benchmark = True\n",
    "            net = torch.nn.DataParallel(model) \n",
    "            model = net.cuda()\n",
    "\n",
    "        # One epoch's training\n",
    "        train(train_loader=dataloader_train, model=model, optimizer=optimizer, epoch=epoch, on_dev = on_dev)\n",
    "        # One epoch's validation\n",
    "        val_loss = validate(val_loader=dataloader_val,model=model, on_dev = on_dev)\n",
    "        \n",
    "        # Did validation loss improve?\n",
    "        is_best = val_loss < best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "            \n",
    "        pp = PrettyPrinter()\n",
    "        APs, mAP = 1, 1\n",
    "        # Save checkpoint\n",
    "        model_Odict = []\n",
    "        for name in model.state_dict():\n",
    "            tensor = model.state_dict()[name].to(torch.device('cpu'))\n",
    "            model_Odict.append((name, tensor))\n",
    "\n",
    "        save_checkpoint(epoch, epochs_since_improvement, model_code, OrderedDict(model_Odict), \n",
    "                    optimizer, learning_rate, val_loss, best_loss, is_best, checkpoint_filename)\n",
    "        del model_Odict\n",
    "        \n",
    "        # measure mAP\n",
    "        last_rez = measure_mAP(model,dataloader_test, data_folder, data_size)\n",
    "        \n",
    "        # dataset resampling\n",
    "        if os.path.exists(os.path.join(data_folder,'mAPs.pkl')):\n",
    "            tmp_df = pd.read_pickle(os.path.join(data_folder,'mAPs.pkl'))\n",
    "            flag = 1\n",
    "            div_coeff = float(data_size.split('_')[-1])\n",
    "            if float(last_rez.mAP[0])>.5:\n",
    "                resample_dataset(0.97, last_rez, div_coeff, wtf=False, random_choice = True)  \n",
    "                \n",
    "            df = pd.read_pickle(os.path.join(data_folder,'mAPs.pkl'))\n",
    "            df.sort_values(by='mAP', ascending=False, inplace=True)\n",
    "            df.head(1).to_pickle(os.path.join(data_folder,'mAPs.pkl'))\n",
    "            df = pd.read_pickle(os.path.join(data_folder,'mAPs.pkl'))\n",
    "            # remove checkpoints with low mAP\n",
    "            for file in os.listdir('chkpnts/'+data_size):\n",
    "                if file not in df.checkpoint.tolist():\n",
    "                    os.remove(os.path.join('chkpnts',data_size,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T11:19:10.058770Z",
     "start_time": "2019-12-17T11:19:02.944290Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset selection\n",
    "folder = './data/scans/'\n",
    "dataFolder_list = os.listdir(folder)\n",
    "dataFolder_list.sort()\n",
    "for i, element in enumerate(dataFolder_list):\n",
    "    print(i, element)\n",
    "    \n",
    "flag = 0\n",
    "avalible_indexes = [i for i in range(len(dataFolder_list))]\n",
    "while flag !=1:\n",
    "    try:\n",
    "        num = int(input())\n",
    "        if num in avalible_indexes:\n",
    "            flag = 1\n",
    "        else:\n",
    "            print('Wrong! try again!')\n",
    "    except:\n",
    "        print('Wrong! try again!')\n",
    "print('\\nDataset',dataFolder_list[num],' selected!')\n",
    "\n",
    "data_size = dataFolder_list[num]\n",
    "print('data_size = ', data_size)\n",
    "data_folder = folder+data_size  # folder with data files(PATHs to images and annotations)\n",
    "print('data_folder = ', data_folder)\n",
    "\n",
    "free_device = 0\n",
    "device = torch.device('cuda:{}'.format(free_device))\n",
    "\n",
    "# NUM classes in dataset\n",
    "n_classes = 9\n",
    "\n",
    "# checking for pretrained models or checkpoints\n",
    "checkpoint_filename = 'BAGS_pretren_bc_'+data_size+'.pth.tar'\n",
    "if os.path.exists('BEST_'+ checkpoint_filename):\n",
    "    checkpoint = 'BEST_'+ checkpoint_filename\n",
    "    print('checkpoint exists')\n",
    "else:\n",
    "    checkpoint = None         \n",
    "    print(\"there isn't any checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T11:19:10.066053Z",
     "start_time": "2019-12-17T11:19:10.061093Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1 #---------------> batch size\n",
    "test_batch_size = 8 #----------> test_batch_size\n",
    "start_epoch = 0 #--------------> start at this epoch\n",
    "epochs = 300 #-----------------> number of epochs to run without early-stopping\n",
    "epochs_since_improvement = 0 #-> number of epochs since there was an improvement in the validation metric\n",
    "best_loss = 100 #--------------> assume a high loss at first\n",
    "workers = 4 #------------------> number of workers for loading data in the DataLoader\n",
    "print_freq = 200 #-------------> print training or validation status every __ batches\n",
    "lr = 1e-3 #--------------------> learning rate\n",
    "momentum = 0.9 #---------------> momentum\n",
    "weight_decay = 5e-4 #----------> weight decay\n",
    "keep_difficult = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T11:33:41.162291Z",
     "start_time": "2019-12-17T11:19:10.068187Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':    \n",
    "    main(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
